const { DocumentProcessorServiceClient } = require('@google-cloud/documentai').v1;
const { BigQuery } = require('@google-cloud/bigquery');
const { Storage } = require('@google-cloud/storage');

const projectId = process.env.PROJECT_ID;
const location = process.env.LOCATION;
const processorId = process.env.PROCESSOR_ID;
const bqTable = process.env.BQ_TABLE;

exports.processPdf = async (event, context) => {
  const file = event.name;
  const bucket = event.bucket;

  if (!file.endsWith('.pdf')) {
    console.log(`‚è≠Ô∏è Skipping non-PDF file: ${file}`);
    return;
  }

  if (!file.startsWith('upload/')) {
    console.log(`‚è≠Ô∏è Skipping file not in upload/ folder: ${file}`);
    return;
  }

  if (!projectId || !location || !processorId || !bqTable) {
    throw new Error("‚ùå Missing one or more required environment variables: PROJECT_ID, LOCATION, PROCESSOR_ID, BQ_TABLE");
  }

  const gcsInputUri = `gs://${bucket}/${file}`;
  const processorName = `projects/${projectId}/locations/${location}/processors/${processorId}`;
  const client = new DocumentProcessorServiceClient();
  const storage = new Storage();

  console.log(`üìÑ New file detected: ${file}`);
  console.log(`üì¶ Document AI Request Payload:`);
  console.log(`‚û°Ô∏è Processor Name: ${processorName}`);
  console.log(`‚û°Ô∏è MIME Type: application/pdf`);
  console.log(`‚û°Ô∏è GCS Input URI: ${gcsInputUri}`);

  try {
    const fileBuffer = (await storage.bucket(bucket).file(file).download())[0];

    const [result] = await client.processDocument({
      name: processorName,
      rawDocument: {
        content: fileBuffer,
        mimeType: 'application/pdf',
      },
    });

    const document = result.document;
    console.log(`‚úÖ Document processed successfully. Found ${document.entities.length} entities.`);

    console.log("üîé Raw Entities from Document:");
    console.log(JSON.stringify(document.entities, null, 2));

    const mapped = {
      PatientName: "",
      DateofBirth: null,
      Gender: "",
      ContactNumber: "",
      Address: "",
      InsuranceProvider: "",
      Symptoms: "",
      DoctorName: "",
      AppointmentDate: null,
      confidence: 0,
      DocName: file,
      ProcessedAt: new Date().toISOString()
    };

    let totalConfidence = 0, count = 0;

    for (const entity of document.entities) {
      const key = entity.type;
      const value = entity.mentionText?.trim();
      const confidence = entity.confidence || 0;

      if (mapped.hasOwnProperty(key)) {
        mapped[key] = value;
        totalConfidence += confidence;
        count++;
      }
    }

    if (count > 0) {
      mapped.confidence = +(totalConfidence / count).toFixed(4);
    }

    console.log("üìù Row to insert into BigQuery:");
    console.log(JSON.stringify(mapped, null, 2));

    const bigquery = new BigQuery();

    try {
      await bigquery
        .dataset(bqTable.split('.')[1])
        .table(bqTable.split('.')[2])
        .insert([mapped]);
      console.log(`‚úÖ Inserted record into BigQuery table: ${bqTable}`);
    } catch (e) {
      if (e.name === 'PartialFailureError') {
        console.error("‚ö†Ô∏è Some rows failed to insert into BigQuery:");
        e.errors.forEach(err => console.error(JSON.stringify(err, null, 2)));
      } else {
        console.error("‚ùå Unexpected BigQuery error:", e.message);
      }
      throw e;
    }

    const archivePath = `archive/${file.split('/').pop()}`;
    await storage.bucket(bucket).file(file).move(archivePath);
    console.log(`üì¶ Archived processed file to: ${archivePath}`);
  } catch (err) {
    console.error("‚ùå Error during processing:");
    console.error(err.message);
    console.error(err.stack);
    throw err;
  }
};
